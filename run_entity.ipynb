{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiangliao/anaconda3/envs/cxr_graph/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yuxiangliao/anaconda3/envs/cxr_graph/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from shared.data_structures import Dataset\n",
    "from shared.const import task_ner_labels, get_labelmap\n",
    "from entity.utils import convert_dataset_to_samples, batchify, NpEncoder\n",
    "from entity.models import EntityModel\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setseed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def check_and_mkdirs(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2023 04:15:07 - INFO - root - ['/home/yuxiangliao/anaconda3/envs/cxr_graph/lib/python3.10/site-packages/ipykernel_launcher.py', '--ip=127.0.0.1', '--stdin=9008', '--control=9006', '--hb=9005', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"16d9b86d-5d24-4fed-896e-3b7d0a1e8f1d\"', '--shell=9007', '--transport=\"tcp\"', '--iopub=9009', '--f=/home/yuxiangliao/.local/share/jupyter/runtime/kernel-v2-91488E1LCHlPUKEI7.json']\n",
      "07/15/2023 04:15:07 - INFO - root - {\"task\": \"scierc\", \"data_dir\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/data/scierc_data/processed_data/json\", \"output_dir\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/PURE/models/ent_scierc_01\", \"use_albert\": false, \"model\": \"allenai/scibert_scivocab_uncased\", \"bert_model_dir\": null, \"do_train\": true, \"do_eval\": true, \"eval_test\": true, \"dev_pred_filename\": \"ent_pred_dev.json\", \"test_pred_filename\": \"ent_pred_test.json\", \"seed\": 0, \"context_window\": 300, \"max_span_length\": 8, \"train_batch_size\": 16, \"eval_batch_size\": 32, \"learning_rate\": 1e-05, \"task_learning_rate\": 0.0005, \"warmup_proportion\": 0.1, \"num_epoch\": 100, \"print_loss_step\": 100, \"eval_per_epoch\": 1, \"bertadam\": false, \"train_shuffle\": false, \"train_data\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/data/scierc_data/processed_data/json/train.json\", \"dev_data\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/data/scierc_data/processed_data/json/dev.json\", \"test_data\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/data/scierc_data/processed_data/json/test.json\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"task\": \"scierc\", \"data_dir\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/data/scierc_data/processed_data/json\", \"output_dir\": \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/PURE/models/ent_scierc_01\", \"use_albert\": false, \"model\": \"allenai/scibert_scivocab_uncased\", \"bert_model_dir\": null, \"do_train\": true, \"do_eval\": true, \"eval_test\": true, \"dev_pred_filename\": \"ent_pred_dev.json\", \"test_pred_filename\": \"ent_pred_test.json\", \"seed\": 0, \"context_window\": 300, \"max_span_length\": 8, \"train_batch_size\": 16, \"eval_batch_size\": 32, \"learning_rate\": 1e-05, \"task_learning_rate\": 0.0005, \"warmup_proportion\": 0.1, \"num_epoch\": 100, \"print_loss_step\": 100, \"eval_per_epoch\": 1, \"bertadam\": false, \"train_shuffle\": false}\n"
     ]
    }
   ],
   "source": [
    "class ArgClass:\n",
    "    def __init__(self):\n",
    "        self.task = \"scierc\"\n",
    "        self.data_dir = (\n",
    "            \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/data/scierc_data/processed_data/json\"\n",
    "        )\n",
    "        self.output_dir = \"/home/yuxiangliao/PhD/workspace/VSCode_workspace/cxr_graph/resources/PURE/models/ent_scierc_01\"\n",
    "\n",
    "        self.use_albert = False  # albert requires different class from the transformers library\n",
    "        self.model = \"allenai/scibert_scivocab_uncased\"\n",
    "        # if args.bert_model_dir is not None, bert_model_name = str(args.bert_model_dir) + \"/\", elif bert_model_name = args.model\n",
    "        self.bert_model_dir = None\n",
    "\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.eval_test = True\n",
    "        self.dev_pred_filename = \"ent_pred_dev.json\"\n",
    "        self.test_pred_filename = \"ent_pred_test.json\"\n",
    "\n",
    "        self.seed = 0\n",
    "        self.context_window = 300\n",
    "        self.max_span_length = 8\n",
    "        self.train_batch_size = 16\n",
    "        self.eval_batch_size = 32\n",
    "        self.learning_rate = 1e-05\n",
    "        self.task_learning_rate = 5e-04\n",
    "        self.warmup_proportion = 0.1\n",
    "        self.num_epoch = 100\n",
    "        self.print_loss_step = 100\n",
    "        self.eval_per_epoch = 1\n",
    "        self.bertadam = False\n",
    "        self.train_shuffle = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps(vars(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "args = ArgClass()\n",
    "print(args)\n",
    "\n",
    "args.train_data = os.path.join(args.data_dir, \"train.json\")\n",
    "args.dev_data = os.path.join(args.data_dir, \"dev.json\")\n",
    "args.test_data = os.path.join(args.data_dir, \"test.json\")\n",
    "\n",
    "if \"albert\" in args.model:\n",
    "    logger.info(\"Use Albert: %s\" % args.model)\n",
    "    args.use_albert = True\n",
    "\n",
    "setseed(args.seed)\n",
    "\n",
    "check_and_mkdirs(args.output_dir)\n",
    "\n",
    "if args.do_train:\n",
    "    logger.addHandler(logging.FileHandler(os.path.join(args.output_dir, \"train.log\"), \"w\"))\n",
    "else:\n",
    "    logger.addHandler(logging.FileHandler(os.path.join(args.output_dir, \"eval.log\"), \"w\"))\n",
    "\n",
    "logger.info(sys.argv)\n",
    "logger.info(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForEntity: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForEntity from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForEntity from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForEntity were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['ner_classifier.1.bias', 'ner_classifier.0._linear_layers.0.bias', 'ner_classifier.0._linear_layers.0.weight', 'ner_classifier.0._linear_layers.1.bias', 'ner_classifier.1.weight', 'ner_classifier.0._linear_layers.1.weight', 'width_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ner_label2id, ner_id2label = get_labelmap(task_ner_labels[args.task])\n",
    "num_ner_labels = len(task_ner_labels[args.task]) + 1  # including null\n",
    "\n",
    "model = EntityModel(args, num_ner_labels=num_ner_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2023 04:15:11 - INFO - root - # Overlap: 0\n",
      "07/15/2023 04:15:11 - INFO - root - Extracted 1861 samples from 350 documents, with 5598 NER labels, 140.335 avg input length, 300 max length\n",
      "07/15/2023 04:15:11 - INFO - root - Max Length: 101, max NER: 13\n"
     ]
    }
   ],
   "source": [
    "train_data = Dataset(args.train_data)\n",
    "train_samples, train_ner = convert_dataset_to_samples(\n",
    "    train_data, args.max_span_length, ner_label2id=ner_label2id, context_window=args.context_window\n",
    ")\n",
    "train_batches = batchify(train_samples, args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2023 04:15:11 - INFO - root - # Overlap: 0\n",
      "07/15/2023 04:15:11 - INFO - root - Extracted 275 samples from 50 documents, with 811 NER labels, 138.455 avg input length, 226 max length\n",
      "07/15/2023 04:15:11 - INFO - root - Max Length: 68, max NER: 11\n"
     ]
    }
   ],
   "source": [
    "dev_data = Dataset(args.dev_data)\n",
    "dev_samples, dev_ner = convert_dataset_to_samples(\n",
    "    dev_data, args.max_span_length, ner_label2id=ner_label2id, context_window=args.context_window\n",
    ")\n",
    "dev_batches = batchify(dev_samples, args.eval_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batches, tot_gold):\n",
    "    \"\"\"\n",
    "    Evaluate the entity model\n",
    "    \"\"\"\n",
    "    logger.info('Evaluating...')\n",
    "    c_time = time.time()\n",
    "    cor = 0\n",
    "    tot_pred = 0\n",
    "    l_cor = 0\n",
    "    l_tot = 0\n",
    "\n",
    "    for i in range(len(batches)):\n",
    "        output_dict = model.run_batch(batches[i], training=False)\n",
    "        pred_ner = output_dict['pred_ner']\n",
    "        for sample, preds in zip(batches[i], pred_ner):\n",
    "            for gold, pred in zip(sample['spans_label'], preds):\n",
    "                l_tot += 1\n",
    "                if pred == gold:\n",
    "                    l_cor += 1\n",
    "                if pred != 0 and gold != 0 and pred == gold:\n",
    "                    cor += 1\n",
    "                if pred != 0:\n",
    "                    tot_pred += 1\n",
    "\n",
    "    acc = l_cor / l_tot\n",
    "    logger.info('Accuracy: %5f'%acc)\n",
    "    logger.info('Cor: %d, Pred TOT: %d, Gold TOT: %d'%(cor, tot_pred, tot_gold))\n",
    "    p = cor / tot_pred if cor > 0 else 0.0\n",
    "    r = cor / tot_gold if cor > 0 else 0.0\n",
    "    f1 = 2 * (p * r) / (p + r) if cor > 0 else 0.0\n",
    "    logger.info('P: %.5f, R: %.5f, F1: %.5f'%(p, r, f1))\n",
    "    logger.info('Used time: %f'%(time.time()-c_time))\n",
    "    return f1\n",
    "\n",
    "def save_model(model, args):\n",
    "    \"\"\"\n",
    "    Save the model to the output directory\n",
    "    \"\"\"\n",
    "    logger.info('Saving model to %s...'%(args.output_dir))\n",
    "    model_to_save = model.bert_model.module if hasattr(model.bert_model, 'module') else model.bert_model\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    model.tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "def output_ner_predictions(model, batches, dataset, output_file):\n",
    "    \"\"\"\n",
    "    Save the prediction as a json file\n",
    "    \"\"\"\n",
    "    ner_result = {}\n",
    "    span_hidden_table = {}\n",
    "    tot_pred_ett = 0\n",
    "    for i in range(len(batches)):\n",
    "        output_dict = model.run_batch(batches[i], training=False)\n",
    "        pred_ner = output_dict['pred_ner']\n",
    "        for sample, preds in zip(batches[i], pred_ner):\n",
    "            off = sample['sent_start_in_doc'] - sample['sent_start']\n",
    "            k = sample['doc_key'] + '-' + str(sample['sentence_ix'])\n",
    "            ner_result[k] = []\n",
    "            for span, pred in zip(sample['spans'], preds):\n",
    "                span_id = '%s::%d::(%d,%d)'%(sample['doc_key'], sample['sentence_ix'], span[0]+off, span[1]+off)\n",
    "                if pred == 0:\n",
    "                    continue\n",
    "                ner_result[k].append([span[0]+off, span[1]+off, ner_id2label[pred]])\n",
    "            tot_pred_ett += len(ner_result[k])\n",
    "\n",
    "    logger.info('Total pred entities: %d'%tot_pred_ett)\n",
    "\n",
    "    js = dataset.js\n",
    "    for i, doc in enumerate(js):\n",
    "        doc[\"predicted_ner\"] = []\n",
    "        doc[\"predicted_relations\"] = []\n",
    "        for j in range(len(doc[\"sentences\"])):\n",
    "            k = doc['doc_key'] + '-' + str(j)\n",
    "            if k in ner_result:\n",
    "                doc[\"predicted_ner\"].append(ner_result[k])\n",
    "            else:\n",
    "                logger.info('%s not in NER results!'%k)\n",
    "                doc[\"predicted_ner\"].append([])\n",
    "            \n",
    "            doc[\"predicted_relations\"].append([])\n",
    "\n",
    "        js[i] = doc\n",
    "\n",
    "    logger.info('Output predictions to %s..'%(output_file))\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(json.dumps(doc, cls=NpEncoder) for doc in js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiangliao/anaconda3/envs/cxr_graph/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_result = 0.0\n",
    "\n",
    "# Returns an iterator over module parameters,\n",
    "# yielding both the name of the parameter as well as the parameter itself.\n",
    "param_optimizer = list(model.bert_model.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in param_optimizer if \"bert\" in n]},\n",
    "    {\"params\": [p for n, p in param_optimizer if \"bert\" not in n], \"lr\": args.task_learning_rate},\n",
    "]\n",
    "\n",
    "# Creating the optimizer and defining hyperparameters\n",
    "# 文章中提到，原优化器adam它的数学公式中是带bias-correct，\n",
    "# 而在官方的bert模型中，实现的优化器bertadam是不带bias-correction的。\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=not(args.bertadam))\n",
    "\n",
    "# Define a scheduler and associate it with an optimiser\n",
    "t_total = len(train_batches) * args.num_epoch\n",
    "# 模型迭代前期用较大的lr进行warmup,后期随着迭代，用较小的lr。\n",
    "# Transformer在训练的初始阶段，输出层附近的期望梯度非常大，\n",
    "# warmup可以避免前向FC层的不稳定的剧烈改变，所以没有warm-up的话模型优化过程就会非常不稳定\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, int(t_total*args.warmup_proportion), t_total)\n",
    "\n",
    "tr_loss = 0\n",
    "tr_examples = 0\n",
    "global_step = 0\n",
    "eval_step = len(train_batches) // args.eval_per_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，在每次训练迭代中，你需要执行以下步骤\n",
    "\n",
    "1. 将输入数据和标签数据传递给模型，计算输出和损失：\n",
    "   1. 前向传播 outputs = model(inputs)\n",
    "   2. 计算损失 loss = criterion(outputs, labels)\n",
    "2. 清零优化器的梯度缓存：optimizer.zero_grad()\n",
    "3. 反向传播，计算参数的梯度：loss.backward()\n",
    "4. 更新模型参数：optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2023 04:15:14 - INFO - root - Training epoch = 0\n",
      " 85%|████████▍ | 99/117 [07:12<01:19,  4.40s/it]07/15/2023 04:22:29 - INFO - root - Epoch=0, iter=99, loss=301.07064\n",
      " 99%|█████████▉| 116/117 [08:37<00:05,  5.08s/it]07/15/2023 04:23:52 - INFO - root - Evaluating...\n",
      "07/15/2023 04:24:17 - INFO - root - Accuracy: 0.981875\n",
      "07/15/2023 04:24:17 - INFO - root - Cor: 0, Pred TOT: 0, Gold TOT: 811\n",
      "07/15/2023 04:24:17 - INFO - root - P: 0.00000, R: 0.00000, F1: 0.00000\n",
      "07/15/2023 04:24:17 - INFO - root - Used time: 25.157210\n",
      "100%|██████████| 117/117 [09:03<00:00,  4.65s/it]\n",
      "07/15/2023 04:24:17 - INFO - root - Training epoch = 1\n",
      " 70%|███████   | 82/117 [05:56<02:36,  4.47s/it]07/15/2023 04:30:19 - INFO - root - Epoch=1, iter=82, loss=37.46203\n",
      " 99%|█████████▉| 116/117 [08:40<00:05,  5.10s/it]07/15/2023 04:32:59 - INFO - root - Evaluating...\n",
      "07/15/2023 04:33:24 - INFO - root - Accuracy: 0.981875\n",
      "07/15/2023 04:33:24 - INFO - root - Cor: 0, Pred TOT: 0, Gold TOT: 811\n",
      "07/15/2023 04:33:24 - INFO - root - P: 0.00000, R: 0.00000, F1: 0.00000\n",
      "07/15/2023 04:33:24 - INFO - root - Used time: 25.169191\n",
      "100%|██████████| 117/117 [09:06<00:00,  4.68s/it]\n",
      "07/15/2023 04:33:24 - INFO - root - Training epoch = 2\n",
      " 56%|█████▌    | 65/117 [04:42<03:37,  4.18s/it]07/15/2023 04:38:11 - INFO - root - Epoch=2, iter=65, loss=19.52562\n",
      " 99%|█████████▉| 116/117 [08:42<00:05,  5.11s/it]07/15/2023 04:42:07 - INFO - root - Evaluating...\n",
      "07/15/2023 04:42:33 - INFO - root - Accuracy: 0.981875\n",
      "07/15/2023 04:42:33 - INFO - root - Cor: 0, Pred TOT: 0, Gold TOT: 811\n",
      "07/15/2023 04:42:33 - INFO - root - P: 0.00000, R: 0.00000, F1: 0.00000\n",
      "07/15/2023 04:42:33 - INFO - root - Used time: 25.221972\n",
      "100%|██████████| 117/117 [09:08<00:00,  4.69s/it]\n",
      "07/15/2023 04:42:33 - INFO - root - Training epoch = 3\n",
      " 23%|██▎       | 27/117 [01:58<06:31,  4.35s/it]"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    for epoch_id in range(args.num_epoch):\n",
    "        logger.info('Training epoch = %d', epoch_id)\n",
    "        if args.train_shuffle:\n",
    "            random.shuffle(train_batches)\n",
    "        for i in tqdm(range(len(train_batches))):\n",
    "            # ner_loss, ner_llh = torch.Size([16, 652, 7]) batch_size,span_num,label_num\n",
    "            output_dict = model.run_batch(train_batches[i], training=True)\n",
    "            loss = output_dict['ner_loss']\n",
    "            loss.backward()\n",
    "            \n",
    "            tr_loss += loss.item() # total loss for this epoch\n",
    "            tr_examples += len(train_batches[i]) # total trained examples for this epoch\n",
    "            global_step += 1\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if global_step % args.print_loss_step == 0:\n",
    "                logger.info('Epoch=%d, iter=%d, loss=%.5f'%(epoch_id, i, tr_loss / tr_examples))\n",
    "                tr_loss = 0\n",
    "                tr_examples = 0\n",
    "\n",
    "            if global_step % eval_step == 0:\n",
    "                f1 = evaluate(model, dev_batches, dev_ner)\n",
    "                if f1 > best_result:\n",
    "                    best_result = f1\n",
    "                    logger.info('!!! Best valid (epoch=%d): %.2f' % (epoch_id, f1*100))\n",
    "                    save_model(model, args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_eval:\n",
    "    args.bert_model_dir = args.output_dir\n",
    "    model = EntityModel(args, num_ner_labels=num_ner_labels)\n",
    "    if args.eval_test:\n",
    "        test_data = Dataset(args.test_data)\n",
    "        prediction_file = os.path.join(args.output_dir, args.test_pred_filename)\n",
    "    else:\n",
    "        test_data = Dataset(args.dev_data)\n",
    "        prediction_file = os.path.join(args.output_dir, args.dev_pred_filename)\n",
    "    test_samples, test_ner = convert_dataset_to_samples(test_data, args.max_span_length, ner_label2id=ner_label2id, context_window=args.context_window)\n",
    "    test_batches = batchify(test_samples, args.eval_batch_size)\n",
    "    evaluate(model, test_batches, test_ner)\n",
    "    output_ner_predictions(model, test_batches, test_data, output_file=prediction_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxr_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
